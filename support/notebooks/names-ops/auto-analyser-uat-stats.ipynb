{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto-Analyser-UAT Stats\n",
    "### Follow these steps to generate statistics of names tested against the auto analyser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./notebook-setup.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the data set\n",
    "#### Default: entire data set of results that haven't been seen\n",
    "*Could be expanded upon to select other subsets of uat jobs / names*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = 'unseen'  # all, unseen\n",
    "uat_type = 'uat_accuracy'  # uat_rejection, uat_accuracy, all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the uat jobs included in the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_set == 'unseen':\n",
    "    if uat_type != 'all':\n",
    "        uat_jobs = UatJobResult.get_jobs_with_unsent_results(uat_type=uat_type)\n",
    "    else:\n",
    "        uat_jobs = UatJobResult.get_jobs_with_unsent_results()\n",
    "elif uat_type != 'all':\n",
    "    uat_jobs = UatJobResult.get_finshed_jobs(uat_type=uat_type)\n",
    "else:\n",
    "    uat_jobs = UatJobResult.get_finshed_jobs()\n",
    "uat_jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the response time averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_request_time = 0.0\n",
    "count = 0\n",
    "for job in uat_jobs:\n",
    "    time = job.get_request_time_avg()\n",
    "    print(time)\n",
    "    total_request_time += time\n",
    "    count += 1\n",
    "total_request_time_avg = total_request_time/count\n",
    "total_request_time_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the pass rate"
   ]
  },
  {
   "source": [
    "#### Overall"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_pass_avgs = 0.0\n",
    "count = 0\n",
    "for job in uat_jobs:\n",
    "    pass_avg = job.get_accuracy()\n",
    "    print(pass_avg)\n",
    "    total_pass_avgs += pass_avg\n",
    "    count += 1\n",
    "total_pass_rate = total_pass_avgs/count\n",
    "total_pass_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "approved_names_total = []\n",
    "approved_names_passed = []\n",
    "rejected_names_total = []\n",
    "rejected_names_passed = []\n",
    "for job in uat_jobs:\n",
    "    # total adds both failed and passed (otherwise it would include err'd names)\n",
    "    if uat_type in ['all', 'uat_accuracy']:\n",
    "        approved_names_total += job.get_names(name_state='APPROVED', result='FAILED')\n",
    "        approved_names_total += job.get_names(name_state='APPROVED', result='PASSED')\n",
    "        approved_names_passed += job.get_names(name_state='APPROVED', result='PASSED')\n",
    "    rejected_names_total += job.get_names(name_state='REJECTED', result='FAILED')\n",
    "    rejected_names_total += job.get_names(name_state='REJECTED', result='PASSED')\n",
    "    rejected_names_passed += job.get_names(name_state='REJECTED', result='PASSED')"
   ]
  },
  {
   "source": [
    "#### Approvals pass rate"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if approved_names_total:\n",
    "    print(f'total: {len(approved_names_total)}')\n",
    "    print(f'passed: {len(approved_names_passed)}')\n",
    "\n",
    "    print(f'pass rate: {len(approved_names_passed)/len(approved_names_total)}')\n",
    "else:\n",
    "    print('No approvals associated with these results')"
   ]
  },
  {
   "source": [
    "#### Rejections pass rate"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'total: {len(rejected_names_total)}')\n",
    "print(f'passed: {len(rejected_names_passed)}')\n",
    "\n",
    "print(f'pass rate: {len(rejected_names_passed)/len(rejected_names_total)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the failed names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "should_approve_failed_names = []\n",
    "should_reject_failed_names = []\n",
    "for job in uat_jobs:\n",
    "    should_reject_failed_names += job.get_names(name_state='REJECTED', result='FAILED')\n",
    "    if uat_type in ['all', 'uat_accuracy']:\n",
    "        should_approve_failed_names += job.get_names(name_state='APPROVED', result='FAILED')"
   ]
  },
  {
   "source": [
    "#### Approval failures"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in should_approve_failed_names:\n",
    "    print(name.name)"
   ]
  },
  {
   "source": [
    "#### Rejection failures"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in should_reject_failed_names:\n",
    "    print(name.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save failed results to excel spreadsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if should_approve_failed_names:\n",
    "    write_to_excel(should_approve_failed_names, f'{uat_type}_failed_to_approve.xlsx')\n",
    "write_to_excel(should_reject_failed_names, f'{uat_type}_failed_to_reject.xlsx')"
   ]
  },
  {
   "source": [
    "## Get the err'd names"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errd_names = []\n",
    "non_timeout_errs = []\n",
    "for job in uat_jobs:\n",
    "    errd_names += job.get_names(result='ERROR')\n",
    "for name in errd_names:\n",
    "    if name.auto_analyse_issue_type != '504':\n",
    "        non_timeout_errs.append(name)\n",
    "        print(name.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_excel(non_timeout_errs, f'{uat_type}_error_names.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update jobs results sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for job in uat_jobs:\n",
    "    job.results_sent = True\n",
    "    job.save()\n",
    "db.session.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create MD file of notebook run\n",
    "**NOTE:** save notebook (i.e. _cmd s_) now to have results show in markdown file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.notebook.kernel.execute('nb_name = \"' + IPython.notebook.notebook_name + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_name = nb_name[:-6]+'.md'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$nb_name\" \"$md_name\"\n",
    "jupyter nbconvert $1 --to markdown --output $2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# NameX Daily Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We need to load in these libraries into our notebook in order to query, load, manipulate and view the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import requests\n",
    "import logging\n",
    "import pandas as pd\n",
    "import csv\n",
    "import json\n",
    "import gspread\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "from datetime import datetime, timedelta, tzinfo, timezone\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from df2gspread import df2gspread as d2g\n",
    "module_path = os.path.join(os.getcwd(), \"nr_duplicates_report\")\n",
    "if module_path not in sys.path:\n",
    "    sys.path.insert(0, module_path)\n",
    "from util.token import get_bearer_token\n",
    "from config import Config\n",
    "\n",
    "%load_ext sql\n",
    "%config SqlMagic.displaylimit = 5\n",
    "%config SqlMagic.style = '_DEPRECATED_DEFAULT'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This will create the connection to the database and prep the jupyter magic for SQL..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "engine = create_engine(Config.SQLALCHEMY_DATABASE_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Daily totals for specified date: Following query, 'current_date - 0' means today, 'current_date - 1' means yesterday, 'current_date - 2' means the day before yesterday..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the number of days we want the report to be run over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_days_nr=int(Config.NUMBER_OF_DAYS_NR)\n",
    "report_start_date=datetime.strftime(datetime.now()-timedelta(number_of_days_nr), '%Y-%m-%d')\n",
    "\n",
    "number_of_days_payment=int(Config.NUMBER_OF_DAYS_PAYMENT)\n",
    "payments_start_date=datetime.strftime(datetime.now()-timedelta(number_of_days_payment), '%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get all duplicate names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT DISTINCT\n",
    "r.id, r.nr_num, r.priority_cd AS priority, r.state_cd AS nr_state, \n",
    "r.submitted_date, r.source, r.previous_request_id AS resubmit,\n",
    "n.name, \n",
    "a.first_name || ' ' || a.last_name AS customer_name, \n",
    "a.phone_number, a.email_address\n",
    "FROM requests r\n",
    "JOIN names n ON r.id = n.nr_id\n",
    "JOIN applicants a ON r.id = a.nr_id\n",
    "WHERE r.submitted_date::date >= :report_start_date\n",
    "AND r.state_cd <> 'PENDING_DELETION'\n",
    "AND r.nr_num NOT LIKE 'NR L%'\n",
    "AND n.choice = 1\n",
    "AND n.name IN (\n",
    "    SELECT n.name\n",
    "    FROM requests r, names n\n",
    "    WHERE r.id = n.nr_id\n",
    "    AND r.submitted_date::date >= :report_start_date\n",
    "    AND r.state_cd NOT IN ('PENDING_DELETION')\n",
    "    GROUP BY n.name\n",
    "    HAVING COUNT(n.name) > 1\n",
    ")\n",
    "ORDER BY n.name;\n",
    "\"\"\"\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    result = conn.execute(sqlalchemy.text(query), {\"report_start_date\": report_start_date})\n",
    "    name_requests = pd.DataFrame(result.fetchall(), columns=result.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get all payments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAY_RELAY_URL = Config.OCP_RELAY_URL + '/payments/pay'\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {get_bearer_token()}\"\n",
    "}\n",
    "params = {\"payments_start_date\": payments_start_date}\n",
    "\n",
    "response = requests.get(PAY_RELAY_URL, params=params, headers=headers)\n",
    "if response.ok:\n",
    "    invoices = response.json()\n",
    "else:\n",
    "    logging.exception(\"Error fetching invoices:\", response.status_code, response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge the Duplicate Names with Postgres Payment information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_frame = name_requests\n",
    "\n",
    "paid_frame = pd.DataFrame(invoices)\n",
    "if invoices:\n",
    "        paid_frame['nr_num']=paid_frame['business_identifier']\n",
    "\n",
    "result_frame = nr_frame\n",
    "if not nr_frame.empty and not paid_frame.empty:\n",
    "        result_frame = pd.merge(nr_frame, paid_frame, how='left', on=['nr_num'])        \n",
    "        result_frame=result_frame.drop(['id','business_identifier','created_on','invoice_number','total','receipt_number'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get all legacy payments from GlobalP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLIN_PAY_RELAY_URL = Config.OCP_RELAY_URL + '/payments/colin'\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {get_bearer_token()}\"\n",
    "}\n",
    "params = {\"payments_start_date\": payments_start_date}\n",
    "response = requests.get(COLIN_PAY_RELAY_URL, params=params, headers=headers)\n",
    "\n",
    "if response.ok:\n",
    "    invoices = response.json()\n",
    "    global_payment_frame = pd.DataFrame(invoices)\n",
    "else:\n",
    "    print(\"Error fetching invoices:\", response.status_code, response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge the Duplicate Names with Global Payment information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_payment_frame.columns= global_payment_frame.columns.astype(str).str.lower()\n",
    "\n",
    "if not result_frame.empty and not global_payment_frame.empty:\n",
    "        result_frame = pd.merge(result_frame, global_payment_frame, how='left', on=['nr_num'])\n",
    "\n",
    "result_filename = os.path.join(os.getcwd(), r'nr_duplicates_report/data/')+'nr_duplicates_' + payments_start_date + '.csv'\n",
    "with open(result_filename, 'w') as f:\n",
    "        if result_frame.empty:\n",
    "                f.write('none')\n",
    "        else:\n",
    "                result_frame.to_csv(f, sep=',', encoding='utf-8', index=False)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the data to google storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Config.ENVIRONMENT == 'prod':\n",
    "    if not result_frame.empty:\n",
    "        # Config data dictionary\n",
    "        dictionary = {   \n",
    "            \"type\": os.getenv('TYPE', ''),\n",
    "            \"project_id\": os.getenv('PROJECT_ID', ''),\n",
    "            \"private_key_id\": os.getenv('PRIVATE_KEY_ID', ''),\n",
    "            \"private_key\": os.getenv('PRIVATE_KEY', ''),\n",
    "            \"client_email\": os.getenv('CLIENT_EMAIL', ''),\n",
    "            \"client_id\": os.getenv('CLIENT_ID', ''),\n",
    "            \"auth_uri\": os.getenv('AUTH_URI', ''),\n",
    "            \"token_uri\": os.getenv('TOKEN_URI', ''),\n",
    "            \"auth_provider_x509_cert_url\": os.getenv('AUTH_PROVIDER_X509_cert_URL', ''),\n",
    "            \"client_x509_cert_url\": os.getenv('CLIENT_X509_CERT_URL', '')\n",
    "        }\n",
    "\n",
    "        with open(os.path.join(os.getcwd(), r'nr_duplicates_report/data/')+\"service_key.json\", \"w\") as outfile:\n",
    "            json.dump(dictionary, outfile)\n",
    "\n",
    "        scope = ['https://www.googleapis.com/auth/spreadsheets']\n",
    "        json_file = os.path.join(os.getcwd(), r'data/')+\"service_key.json\"\n",
    "        credentials = ServiceAccountCredentials.from_json_keyfile_name(json_file, scope)\n",
    "        gc = gspread.authorize(credentials)\n",
    "\n",
    "        wks_name = 'Day - ' + datetime.strftime(datetime.now()-timedelta(number_of_days_payment), '%d')\n",
    "        spreadsheet_key = '1KFo3oUyzXo9A1aAOSy8cjR5ArxVT2Uvgdbe8NEZNLJU'\n",
    "        sheet = d2g.upload(result_frame, spreadsheet_key, wks_name, credentials=credentials, col_names=True, row_names=False) \n",
    "else:\n",
    "    logging.info('Skipping upload to Google Storage')"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "vscode": {
   "interpreter": {
    "hash": "fcb35bce15c55b4cacb5112e543368f86c7f98ed17acd45e6841ee83ed1df6e3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
